# 분산 학습 ( Distributed Training )

## 목차

#### 1. 분산 학습이란?

#### 2. 분산 학습 종류

#### 3. 분산 학습에 사용되는 알고리즘

#### 4. 플랫폼에서 활용하기 좋은 분산 학습 기법

#### 5. 분산 학습 성능 분석 ( Ex) Horovod )

#### 6. 플랫폼 분산 학습 기능의 방향성 (결론)

<br/><br/>
결론은 먼저 간단하게 정리하자면,<br/>
  1. 대부분의 딥러닝 & 머신 러닝 Framework 에서는 분산 학습 기능을 제공하고 있다. 기존 코드에서 수정을 최소화하여 작성할 수 있도록 잘 정리되어 있다.

  2. 분산 학습 방법 중 "데이터 병렬화 기법" 이 보편적인 모델에 분산 학습을 지원하기 용이하다.

  3. 실제로 분산 학습을 실행하기 위해서는 분산 학습이 가능한 코드를 작성하는 것도 중요하지만, 분산 학습을 실행할 수 있는 환경을 준비하는 것이 중요하다. **플랫폼에서 분산 학습 기능을 제공한다는 것은, 사용자가 분산 학습 코드를 작성해서 실행하면, 플랫폼이 관련 환경을 자동으로 제공하여 사용자가 코드 작성에만 집중할 수 있도록 만드는 것**이라고 생각한다.
<br/><br/>

<hr />
<br/>

## 1. 분산 학습이란?

#### 분산 학습 ( Distributed Training, Parallel Training )
= 다수의 GPU, 또는 다수의 Worker Node 를 사용하여 Machine Learning, Deep Learning Model 을 학습시키는 방법을 의미한다. 최근 데이터의 규모가 엄청나게 커지고, 관련 Model 의 크기 자체도 크게 증가함에 따라서 기존의 단일 컴퓨터 (Single Node) 및 한 개의 GPU를 사용한 학습이 여러 문제를 겪게 되었다.

1. 모델 자체의 규모가 커지면서 모델 parameter 들을 한 개의 GPU 장치에 저장하여 계산하는 것이 불가능한 경우가 발생하게 되었다.



    - 위 글은 Microsoft의 자연어 처리 모델, T-NLG 에 관한 기사의 일부이다. 일부를 인용해 보면, “13억개의 파라미터를 실행시킬 수 있는 GPU는 존재하지 않기 때문에 병렬화가 필요하다.” 라고 설명하고 있다.
    - 실제로 저렇게 거대한 모델을 사용하지 않더라도, 일반적인 비싸지 않은 GPU에서 학습할 수 없는 모델도 분명히 존재한다.<br/>
<br/>



2. 학습시켜야 할 데이터셋의 규모가 커지면서, 모델 학습 전체 시간이 일, 때로는 주 단위까지 증가하면서 학습이 너무 오래 걸리는 문제가 발생하게 되었다.

= 따라서 다음의 문제를 해결하기 위해서

  1. 모델 자체를 다수의 GPU로 분할하여 전달하고, 학습을 진행하는 방법 ( 모델 병렬화 기법 ).

  2. 전체 데이터셋을 n개로 나누어 각 노드에서 분할 학습하는 방법 ( 데이터 병렬화 기법 ).

등이 고안되었다.

<br/>
<br/>
<br/>

## 2. 분산 학습 종류

= 분산 학습은 크게 2종류가 있다.

1. 모델 병렬화 기법 ( Model Parallelism )
2. 데이터 병렬화 기법 ( Data Parallelism )

#### 1. 모델 병렬화 기법 ( Model Parallelism )

![image](https://user-images.githubusercontent.com/71695489/127615739-657ad2bc-bed2-4989-8dd4-4376be49fece.png)

= 규모가 큰 모델을 ( Parameters 수가 많음. ) 여러 부분으로 나누어 각 부분마다 각각의 GPU 장치에서 실행하는 방식이다. 예를 들어, 딥러닝의 경우 Layer의 수가 많은데, 위의 그림 처럼 주어진 모델의 각 부분의 학습을 각각의 Machine 이 담당하는 방식이다.

= 모델 병렬화 기법의 중요한 특징은, 장치 간에 파라미터를 전달하는 부분에서 장치간 통신이 발생한다는 것이다. 해당 통신이 빈번하게 발생하면, 통신에 사용되는 시간과 자원의 소모가 커지게 되고, 따라서 이런 통신을 최적화하기 위해서 모델 별로 분할 방식이 다르다.
<br/><br/>

#### 2. 데이터 병렬화 기법 ( Data Parallelism )

![image](https://user-images.githubusercontent.com/71695489/127615751-22bda080-204c-43a5-a5d4-de5198a3c463.png)

= 전체 데이터셋을 n 개의 더 작은 데이터셋의 집합으로 나누어 각각의 GPU device 에서 분할 학습하는 전략이다. 기본적으로 딥러닝에서 전체 데이터셋을 여러 개의 batches 로 나누어 순서대로 학습하는 것을 n개의 GPU 장치에서 각각 한번에 실행한다고 볼 수 도 있다.

= 위 그림을 예시로 들면, 전체 데이터셋을 3개로 분할하여, 3 개의 Worker Nodes 에서 각각 5개의 batches 를 학습하는 것이다. 각각의 노드에서 각각 1개의 batch를 학습한다면, 실제로 3개의 batches 씩 학습하는 효과를 기대할 수 있다.

= 데이터 병렬화 기법에서는 별도로 5개 batches 씩 모두 학습하는 것이 아닌, 각각 1개 batches 씩 학습하고, 학습한 모델의 파라미터를 공유하는 과정 ( 각각 학습한 모델 파라미터를 하나로 합치는 과정 )을 거친다. 위의 경우에는 총 5번 파라미터를 공유하고, 최종적으로는 각각의 노드에서 동일한 모델 파라미터를 얻을 수 있게 된다.

= 학습한 모델의 파라미터를 합치는 방법에 대한 알고리즘은 대표적으로 크게 2가지가 있다.

1. All-reduce 알고리즘
2. ring All-reduce 알고리즘

  1번의 "All-reduce" 알고리즘의 경우 구현이 쉽고 노드의 수가 적을 경우 효과적이다. 2번의 경우 많은 수의 노드 간 파라미터를 공유할 경우에 효과적이다.

<br/><br/>


## 3. 분산 학습에 사용되는 알고리즘

#### 1. All-reduce 알고리즘

![image](https://user-images.githubusercontent.com/71695489/127615775-887b8917-a03d-41c6-b4d1-86daca41334f.png)

= All-reduce 방식은 한 개의 Worker Node 를 Master Node 로 지정하여, 각각의 노드에서 학습한 모델 파라미터, 혹은 Gradient 값을 받아와서 종합하고, 종합된 파라미터 또는 Gradient 값을 다시 분배하는 방식을 이야기한다.

= 총 n개의 Node 에서 All-reduce 방식으로 파라미터를 공유한다고 했을 때, 전체 통신량은 다음과 같다. (k : 비례상수, n : 노드의 개수, s : 모델의 크기)

```math
2 × k × (n-1) × s
```
<br/>
<br/>

#### 2. ring All-reduce 알고리즘

![image](https://user-images.githubusercontent.com/71695489/127615795-4fb906b8-28fd-4a6d-bae5-3560c6332254.png)

= 분산 학습을 지원하는 모든 Framework 들이 대표적으로 사용하는 파라미터 공유 알고리즘이다. 대표적으로 Nvidia의 NCCL 방식이 “ring All-reduce” 방식을 사용하고 있고, 대부분의 프레임워크에서 NCCL 라이브러리를 사용하고 있다.

= 방식에 대한 설명은 다음과 같다.

1. 각 Worker의 파라미터 또는 Gradient 를 Worker의 수 n개로 분할하고 각각을 1, 2, … n 번째 파트로 명명한다.

2. 처음에는 1번 Worker 는 1번 파트를, 2번 Worker 는 2번 Part를, 3번 Worker는 3번 Part를 다음 Worker 에게 전달한다.

3. 다음에는 1번 Worker 는 마지막 n번 Worker 에게 전달받은 n번 파트와 자신의 n번 파트를 결합하여 다음 Worker 에게 전달한다. 마찬가지로 2번 Worker 는 2) 에서 1번 Worker 에게 전달 받은 1번 파트와 자신의 1번 파트를 결합하여 다음 Worker 에게 전달한다.

4. 그 다음에는 마찬가지로 1번 Worker는 n-1 번 Worker 에서 출발해 n번 Worker 를 거쳐 도달한 n-1번 파트와 자신의 n-1번 파트를 결합하여 다음 Worker 에게 전달한다. 다른 Worker 들도 동일한 방식으로 자신의 파트를 결합하여 다음 Worker 에게 전달한다.

5. 총 n번 위 과정을 거치고 나면, 모든 노드의 파라미터는 모든 노드에서 계산된 값이 반영된 동일한 파라미터를 가지게 된다.
<br/>
<br/>
<br/>

## 4. 플랫폼에서 홀용하기 좋은 분산 학습 기법

= 모델 병렬화 기법은 모델 별로 별도의 병렬화 분산학습 코드를 작성해야 한다. 모델 별로 학습 및 추론 과정에서 많이 사용하고, 또는 적게 사용하는 노드가 전부 제각각이기 때문이다.

= 따라서 플랫폼에서 모델 병렬화 기법을 통해서 분산 학습 기능을 제공하는 것은 어렵다. 반대로 데이터 병렬화 기법은 모델은 변경하지 않고, 전체 데이터셋만 분할해서 전달하면 되므로, 데이터 병렬화 기법을 활용할 필요가 있다.

= “데이터 병렬화 기법”을 활용한 분산 학습은 대부분의 머신러닝 & 딥러닝 Framework 에서 잘 제공하고 있다. (Tensorflow, Pytorch, Pytorch Lightning, Horovod 등...)

= 그 중 Horovod 의 성능이 좋은 것으로 평가되고 있고, 코드 수정이 간단하기 때문에 앞으로의 예제에서는 Horovod 를 사용한 분산 학습을 활용하여 분산 학습 기능을 제공하는 것을 목표로 한다.

= 하지만, 대부분의 Framework 에서 분산 학습을 실행하기 위해서 기존의 코드를 일부 변경하는 방식과, 변경 해야 할 코드 등이 유사하기 때문에 한번 변경 사항에 대해서 잘 이해한다면, 다른 Framework 를 사용하고 싶은 경우에 조금 더 쉽게 분산 학습 기능 적용이 가능하다고 본다.
<br/>
<br/>
<br/>

## 5. 분산 학습 성능 분석 ( Ex) Horovod )

=  “데이터 병렬 전략" 을 활용한 분산 학습은 학습 시간을 줄이는 것을 목표로 한다. 따라서 목표로 한 Horovod 분산 학습을 적용했을 때, 실제로 효과가 있는지 확인할 필요가 있다.

=  다음 실험에서는 실제로 Horovod 분산 학습 기능을 활용하여 각각 1개의 GPU가 있는 2개의 Worker Node 에서 MNIST 데이터셋을 가지고 CNN 모델을 학습하고, 학습 시간을 비교했다.

=  실험 환경은 다음과 같다.

  - 2 Worker Nodes, Ubuntu 18.04 with Tesla T4 GPUs (1 GPU for each worker)
  - 5.22 Gbits / sec Network Speed Between two servers.
  - MNIST Datasets. No restriction of the Size.(500 steps per each epoch)
  - Simple Sequential Keras Model.(About 1,200,200 parameters)
  - Fixed Epochs(=24), Variable batch-size(from 1 to 6144)

= Source Code 는 다음에서 찾아볼 수 있다.

- http://lab.t3q.co.kr:9999/kaist-co-op/Distributed-Training/-/blob/master/DistributeTrainingDemo/DistributeTrain_BaseImage/Train.py

= 측정한 학습 시간은 다음과 같다. 이에 대한 그래프는 다음과 같다.

| batch      | 1      | 2      | 4      | 8      | 16     | 32     | 64     | 128    | 256    | 512    | 1024   | 2048    | 3072    | 4096    | 5120    | 6144    |
|------------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|--------|---------|---------|---------|---------|---------|
| SingleNode | 50.14  | 64.33  | 64.31  | 61.61  | 60.7   | 71.3   | 106.84 | 127.19 | 206.69 | 316.7  | 579.65 | 1082.88 | 1706.68 | 2223.04 | 2655.5  | 3343.35 |
| MultiNode0 | 127.46 | 130.01 | 134.54 | 133.77 | 126.9  | 129.41 | 138.15 | 161.43 | 184.32 | 265.02 | 395.19 | 625.72  | 932.34  | 1189.98 | 1449.18 | 1821.45 |
| MultiNode1 | 117.95 | 121.87 | 124.38 | 125.37 | 125.98 | 124.44 | 132.53 | 149.61 | 182.06 | 258.81 | 388.33 | 623.27  | 926.76  | 1185.42 | 1444.59 | 1811.7  |

![image](https://user-images.githubusercontent.com/71695489/127615842-7f4d9edf-9ea0-4d1d-a8b6-1ced5e389750.png)

=  실험 결과를 보면, 128 batch-size 이하는 Single-Node 에서 더 빠른 속도를, 256 batch-size 이상에서는 Multi-node 에서 더 빠른 학습 속도를 보임을 확인할 수 있다.

=  결과에 대한 분석으로는 다음과 같다.

1. 실험에서 Epoches = 24, Steps = 500 으로 고정했다. ( 파라미터 결합은 500번  24 epochs 만큼 이뤄진다. ) 따라서 분산 학습을 위한 노드 간 Parameter 를 공유하는 과정에서 사용되는 시간 비용은 모든 과정에서 동일하다. (상수)

2. 실험에서 batch-size 를 변량으로 설정하였고, batch size 를 키울 때마다 전체 학습 시간 및 학습량이 증가한다. 따라서 batch-size 를 키울 수록 전체 학습 시간에서 분산 학습에 사용되는 시간에 대한 비율이 감소한다.

= 따라서 분산 학습은 다음의 경우에 일반적으로 효과적이다.

1. 모델의 크기가 크고, 파라미터 수가 많아 일반적으로 학습 시간이 오래 걸리는 경우.

2. batch-size 를 키워서 GPU 이용률을 높여서 학습을 진행하는 경우.

= 또한, 분산 학습을 위해서 노드 개수를 무조건적으로 늘리는 경우에, 파라미터 통신을 위한 시간 소모가 커지기 때문에, 노드의 갯수를 줄이고, 노드 당 GPU의 개수를 늘려서 학습을 진행할수록 분산 학습 성능을 더욱 높일 수 있다.
<br/>
<br/>
<br/>

## 6. 플랫폼 분산 학습 기능의 방향성 (결론)

= 데이터 병렬화 기법, 그리고 그 과정에서 사용되는 대표적인 알고리즘 들은 Tensorflow, Pytorch 등의 머신 러닝 & 딥러닝 Framework 에서 추가 기능으로 제공하고 있다. 또한, 분산 학습만을 위한 Framework 도 전부 코드 수정을 최소화하여 개발자가 학습 코드 작성을 완성하면, 일부분만 수정해서 분산 학습이 가능하도록 제공하고 있다.

1) Tensorflow 분산 학습 관련 출처
  - https://www.tensorflow.org/guide/distributed_training

2) Pytorch 분산 학습 관련 출처
  - https://pytorch.org/tutorials/advanced/rpc_ddp_tutorial.html#combining-distributed-dataparallel-with-distributed-rpc-framework

3) Pytorch Lightning 분산 학습 관련 출처
  - https://pytorch-lightning.readthedocs.io/en/latest/advanced/multi_gpu.html?highlight=distributed#distributed-data-parallel

4) Horovod 분산 학습 관련 출처
  - https://horovod.readthedocs.io/en/stable/

<br/>

= 하지만 이들 Framework 들에서 제공하는 방식을 통해서 분산학습을 실행하면 잘 작동하지 않는다. 분산 학습을 실행하기 위해서는 크게 2가지 조건을 충족시켜야 하기 때문이다.

1. 분산 학습을 진행하기 위한 Multi-node, Multi-gpu 환경 및 이들이 유연하게 통신하기 위한 사전 환경 준비.

2. 사전에 준비된 환경에서 Multi-node 환경을 활용할 수 있는 코드 작성.

<br/>

= Framework 들은 2번 항목에 대한 해법을 잘 제공하고 있다. 그러나 1번 항목은 개발자가 직접 관련 환경을 미리 준비해주어야 한다. 딥러닝 개발 경험이 있는 사람들이라면, 딥러닝 실행을 위한 환경 설정 (GPU 연결 및 관련 프레임워크 설치, 버전 관리 등...) 이 쉬운 작업이 아니라는 것을 알 수 있다.

<br/>
<br/>

= 분산 학습은 딥러닝 Framework, 설치에 더하여 분산 학습을 위한 추가적인 라이브러리 및 툴들을 설치해야 한다. 그리고 그 과정에서 서로 맞는 버전들을 활용해야 하고, 모든 노드 (컴퓨터) 에 각각 전부 설치해야 하며, 분산 학습을 진행하기 위해 각각의 노드들의 포트를 열어놓고, 연결을 허용하는 과정 또한 필요하다. => 쉽지 않다!

<br/>
<br/>

= 플랫폼에서는 이러한 환경을 잘 제공하여, 개발자가 기존의 단일 노드의 학습 코드를 작성하고, 일부 코드만 수정하면, 환경 제공과 관련된 모든 사항을 제공함으로써, 기존의 학습 과정과 유사하고, 일부 코드에 대한 조금의 수정만 추가하여 분산 학습을 손쉽게 할 수 있는데 목적이 있다.

<br/>
<br/>

= 여기에 더해서, 현재 플랫폼에서는 쿠버네티스 리소스 및 도커 컨테이너를 활용하여 AI 플랫폼 기능을 제공하고 있다. 현재는 Horovod 를 활용한 Base Image 를 가지고 컨테이너를 생성하여 분산 학습을 제공하고 있지만, Pytorch Lightning 등 다른 분산학습 기능을 사용하는 것을 희망 한다면, 관련 Framework 및 Library 를 설치하는 Base Image 를 잘 정의한다면 마찬가지로 분산학습 기능을 활용할 수 있다.
